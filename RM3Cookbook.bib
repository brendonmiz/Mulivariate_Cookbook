Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Krishnan2011,
abstract = {Partial Least Squares (PLS) methods are particularly suited to the analysis of relationships between measures of brain activity and of behavior or experimental design. In neuroimaging, PLS refers to two related methods: (1) symmetric PLS or Partial Least Squares Correlation (PLSC), and (2) asymmetric PLS or Partial Least Squares Regression (PLSR). The most popular (by far) version of PLS for neuroimaging is PLSC. It exists in several varieties based on the type of data that are related to brain activity: behavior PLSC analyzes the relationship between brain activity and behavioral data, task PLSC analyzes how brain activity relates to pre-defined categories or experimental design, seed PLSC analyzes the pattern of connectivity between brain regions, and multi-block or multi-table PLSC integrates one or more of these varieties in a common analysis. PLSR, in contrast to PLSC, is a predictive technique which, typically, predicts behavior (or design) from brain activity. For both PLS methods, statistical inferences are implemented using cross-validation techniques to identify significant patterns of voxel activation. This paper presents both PLS methods and illustrates them with small numerical examples and typical applications in neuroimaging. {\textcopyright} 2010 Elsevier Inc.},
author = {Krishnan, Anjali and Williams, Lynne J. and McIntosh, Anthony Randal and Abdi, Herv{\'{e}}},
doi = {10.1016/j.neuroimage.2010.07.034},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/A81-PLS method for neuroimaging-a tutorial and review.pdf:pdf},
issn = {10538119},
journal = {NeuroImage},
keywords = {Asymmetric PLS,Barycentric discriminant analysis,Behavior PLS,Canonical variate analysis,Co-inertia analysis,Common factor analysis,Multi-block PLS,Multi-table PLS,Multiple factor analysis,PLS,Partial least squares correlation,Partial least squares path modeling,Partial least squares regression,STATIS,Seed PLS,Symmetric PLS,Task PLS},
number = {2},
pages = {455--475},
title = {{Partial Least Squares (PLS) methods for neuroimaging: A tutorial and review}},
volume = {56},
year = {2011}
}
@article{Pele2011,
abstract = {Class A G-protein-coupled receptors (GPCRs) constitute the largest family of transmembrane receptors in the human genome. Understanding the mechanisms which drove the evolution of such a large family would help understand the specificity of each GPCR sub-family with applications to drug design. To gain evolutionary information on class A GPCRs, we explored their sequence space by metric multidimensional scaling analysis (MDS). Three-dimensional mapping of human sequences shows a non-uniform distribution of GPCRs, organized in clusters that lay along four privileged directions. To interpret these directions, we projected supplementary sequences from different species onto the human space used as a reference. With this technique, we can easily monitor the evolutionary drift of several GPCR sub-families from cnidarians to humans. Results support a model of radiative evolution of class A GPCRs from a central node formed by peptide receptors. The privileged directions obtained from the MDS analysis are interpretable in terms of three main evolutionary pathways related to specific sequence determinants. The first pathway was initiated by a deletion in transmembrane helix 2 (TM2) and led to three sub-families by divergent evolution. The second pathway corresponds to the differentiation of the amine receptors. The third pathway corresponds to parallel evolution of several sub-families in relation with a covarion process involving proline residues in TM2 and TM5. As exemplified with GPCRs, the MDS projection technique is an important tool to compare orthologous sequence sets and to help decipher the mutational events that drove the evolution of protein families. {\textcopyright} 2011 Pel{\'{e}} et al.},
author = {Pel{\'{e}}, Julien and Abdi, Herv{\'{e}} and Moreau, Matthieu and Thybert, David and Chabbert, Marie},
doi = {10.1371/journal.pone.0019094},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/A83-Multidimensional scaling reveals the main evolutionary pathways of Class A G-protein-coupled receptors.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {4},
pages = {1--10},
title = {{Multidimensional scaling reveals the main evolutionary pathways of class A G-protein-coupled receptors}},
volume = {6},
year = {2011}
}
@incollection{Abdi2007b,
address = {Thousand Oaks},
author = {Abdi, Herv{\'{e}}},
booktitle = {Encyclopedia of Measurement and Statistics},
chapter = {Distance},
editor = {Salkind, Neil},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/C49-Distance.pdf:pdf},
pages = {39--85},
publisher = {Sage},
title = {{Distance}},
volume = {307},
year = {2007}
}
@article{Cohen1994,
author = {Cohen, Jacob},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/Cohen{\_}TheEarthIsRound.pdf:pdf},
journal = {American Psychologist1},
number = {12},
pages = {997--1003},
title = {{TheEarthIsRound (p{\textless}.05)}},
volume = {49},
year = {1994}
}
@article{Abdi2010a,
abstract = {Partial least squares (PLS) regression (a.k.a. projection on latent structures) is a recent technique that combines features from and generalizes principal component analysis (PCA) and multiple linear regression. Its goal is to predict a set of dependent variables from a set of independent variables or predictors. This prediction is achieved by extracting from the predictors a set of orthogonal factors called latent variables which have the best predictive power. These latent variables can be used to create displays akin to PCA displays. The quality of the prediction obtained from a PLS regression model is evaluated with cross-validation techniques such as the bootstrap and jackknife. There are two main variants of PLS regression: The most common one separates the roles of dependent and independent variables; the second one—used mostly to analyze brain imaging data—gives the same roles to dependent and independent variables .},
author = {Abdi, Herv{\'{e}}},
doi = {10.1002/wics.051},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/A76-PLS Regression.pdf:pdf},
isbn = {1939-0068},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
number = {1},
pages = {97--106},
title = {{Partial least squares regression and projection on latent structure regression (PLS Regression) Wiley Interdisciplinary Reviews: Computational Statistics Volume 2, Issue 1}},
url = {https://www.utdallas.edu/{~}herve/abdi-wireCS-PLS2010.pdf{\%}0Ahttp://onlinelibrary.wiley.com/doi/10.1002/wics.51/abstract},
volume = {2},
year = {2010}
}
@incollection{Abdi2018a,
abstract = {This work deals with the use of multiple correspondence analysis (MCA) and a weighted Euclidean distance (the tolerance distance) as an exploratory tool in developing predictive logistic models. The method was applied to a living-donor kidney transplant data set with 109 cases and 13 predictors. This approach, followed by backward and forward selection procedures, yielded two models, one with four and another with two predictors. These models were compared to two other models, ordinarily built by backward and forward stepwise selection, which yielded, respectively, five and two predictors. After internal validation, the models performance statistics showed similar results. Likelihood ratio tests suggested that backward approach achieved a better fit than the forward modelling in both methods and the Vuong's non-nested test between backward-built models suggested that these were undistinguishable. We conclude that the tolerance distance, in combination with MCA, could be a feasible method for variable selection in logistic modelling, when there are several categorical predictors.},
address = {Thousand Oaks},
author = {Abdi, Herv{\'{e}}},
booktitle = {Multiple Correspondence Analysis},
doi = {10.4324/9781315516257-3},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/C41-Multiple correspondence analysis.pdf:pdf},
pages = {31--55},
publisher = {Sage},
title = {{Multiple correspondence analysis}},
year = {2018}
}
@misc{Abdi2018b,
author = {Abdi, Herv{\'{e}} and B{\'{e}}ra, Michel},
booktitle = {Encyclopedia of Social Networks and Mining},
doi = {10.1007/978-3-642-04898-2_195},
edition = {2nd},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/C85-Correspondence analysis.pdf:pdf},
pages = {275 -- 284},
publisher = {Springer Verlag},
title = {{Correspondence Analysis}},
year = {2018}
}
@article{Berry2011,
abstract = {Permutation tests are a paradox of old and new. Permutation tests pre-date most traditional parametric statistics, but only recently have become part of the mainstream discussion regarding statistical testing. Permutation tests follow a permutation or 'conditional on errors' model whereby a test statistic is computed on the observed data, then (1) the data are permuted over all possible arrangements of the data-an exact permutation test; (2) the data are used to calculate the exact moments of the permutation distribution-a moment approximation permutation test; or (3) the data are permuted over a subset of all possible arrangements of the data-a resampling approximation permutation test. The earliest permutation tests date from the 1920s, but it was not until the advent of modern day computing that permutation tests became a practical alternative to parametric statistical tests. In recent years, permutation analogs of existing statistical tests have been developed. These permutation tests provide noteworthy advantages over their parametric counterparts for small samples and populations, or when distributional assumptions cannot be met. Unique permutation tests have also been developed that allow for the use of Euclidean distance rather than the squared Euclidean distance that is typically employed in parametric tests. This overview provides a chronology of the development of permutation tests accompanied by a discussion of the advances in computing that made permutation tests feasible. Attention is paid to the important differences between 'population models' and 'permutation models', and between tests based on Euclidean and squared Euclidean distances. WIREs Comp Stat 2011 3 527-542 DOI: 10.1002/wics.177 For further resources related to this article, please visit the WIREs website. Copyright {\textcopyright} 2011 John Wiley {\&} Sons, Inc..},
author = {Berry, Kenneth J. and Johnston, Janis E. and Mielke, Paul W.},
doi = {10.1002/wics.177},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/Permutation{\_}Berry.pdf:pdf},
issn = {19395108},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
keywords = {Euclidean distance,Exact tests,Permutation tests,Resampling},
number = {6},
pages = {527--542},
title = {{Permutation methods}},
volume = {3},
year = {2011}
}
@misc{Abdi2015,
author = {Abdi, Herv{\'{e}} and Guillemot, Vincent and Eslami, Aida and Beaton, Derek},
booktitle = {Encyclopedia of Social Networks and Mining},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/C87 - CCA.pdf:pdf},
pages = {265--304},
publisher = {2nd},
title = {{Canonical correlation analysis}},
year = {2018}
}
@misc{Abdi2013a,
author = {Abdi, Herv{\'{e}} and Williams, Lynne J.},
booktitle = {Coputational Toxicology},
chapter = {Partial Le},
doi = {10.1007/978-1-62703-059-5},
editor = {Reisfeld, Brad and Mayeno, Arthur N.},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/C75-PLSC and PLSR.pdf:pdf},
isbn = {9781627030595},
pages = {1453--1454},
publisher = {Springer Science+Business Media, LLC},
title = {{Partial Least Squares Methods: Partial Least Squares Correlation and Partial Least Square Regression}},
volume = {II},
year = {2013}
}
@misc{Abdi2018,
author = {Abdi, Herv{\'{e}} and Williams, Lynne J. and B{\'{e}}ra, Michel},
booktitle = {Encyclopedia of Social Network Analysis and Mining},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/C86-BADA.pdf:pdf},
title = {{Barycentric Discriminant Analysis}},
year = {2018}
}
@article{Abdi2012,
abstract = {STATIS is an extension of principal component analysis (PCA) tailored to handle multiple data tables that measure sets of variables collected on the same observations, or, alternatively, as in a variant called dual-STATIS, multiple data tables where the same variables are measured on different sets of observations. STATIS proceeds in two steps: First it analyzes the between data table similarity structure and derives from this analysis an optimal set of weights that are used to compute a linear combination of the data tables called the compromise that best represents the information common to the different data tables; Second, the PCA of this compromise gives an optimal map of the observations. Each of the data tables also provides a map of the observations that is in the same space as the optimum compromise map. In this article, we present STATIS, explain the criteria that it optimizes, review the recent inferential extensions to STATIS and illustrate it with a detailed example. We also review, and present in a common framework, the main developments of STATIS such as (1) X-STATIS or partial triadic analysis (PTA) which is used when all data tables collect the same variables measured on the same observations (e.g., at different times or locations), (2) COVSTATIS, which handles multiple covariance matrices collected on the same observations, (3) DISTATIS, which handles multiple distance matrices collected on the same observations and generalizes metric multidimensional scaling to three way distance matrices, (4) Canonical-STATIS (CANOSTATIS), which generalizes discriminant analysis and combines it with DISTATIS to analyze multitable discriminant analysis problems, (5) power-STATIS, which uses alternative criteria to find STATIS optimal weights, (6) ANISOSTATIS, which extends STATIS to give specific weights to each variable rather than to each whole table, (7) (K + 1)-STATIS (or external-STATIS), which extends STATIS (and PLS-methods and Tucker inter battery analysis) to the analysis of the relationships of several data sets and one external data set, and (8) double-STATIS (or DO-ACT), which generalizes (K + 1)-STATIS and analyzes two sets of data tables, and STATIS-4, which generalizes double-STATIS to more than two sets of data. {\textcopyright} 2012 Wiley Periodicals, Inc.},
author = {Abdi, Herv{\'{e}} and Williams, Lynne J. and Valentin, Domininique and Bennani-Dosse, Mohammed},
doi = {10.1002/wics.198},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/abdi{\_}Wires{\_}AWVB2012{\_}Final.pdf:pdf},
issn = {19395108},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
keywords = {ANISOSTATIS,Anisotropic STATIS,Barycentric discriminant analysis, generalized sin,COSTATIS,COVSTATIS,Canonical-STATIS,Co-inertia analysis,Consensus PCA,Correspondence analysis,DISTATIS,Discriminant analysis,Double-STATIS,Dual-STATIS,INDSCAL,Multiblock,Multiblock PCA,Multiblock barycentric,Multidimensional scaling,Multiple factor analysis,Multitable PCA,PTA,Partial triadic analysis,Power-STATIS,Principal component analysis,R V-PCA,STATICO,STATIS,STATIS-4,SUM-PCA,X-STATIS},
number = {2},
pages = {124--167},
title = {{STATIS and DISTATIS: Optimum multitable principal component analysis and three way metric multidimensional scaling}},
volume = {4},
year = {2012}
}
@article{Abdi2009,
abstract = {When used to analyze brain imaging data, pattern classifiers typically produce results that can be interpreted as a measure of discriminability or as a distance between some experimental categories. These results can be analyzed with techniques such as multidimensional scaling (MDS), which represent the experimental categories as points on a map. While such a map reveals the configuration of the categories, it does not provide a reliability estimate of the position of the experimental categories, and therefore cannot be used for inferential purposes. In this paper, we present a procedure that provides reliability estimates for pattern classifiers. This procedure combines bootstrap estimation (to estimate the variability of the experimental conditions) and a new 3-way extension of MDS, called DISTATIS, that can be used to integrate the distance matrices generated by the bootstrap procedure and to represent the results as MDS-like maps. Reliability estimates are expressed as (1) tolerance intervals which reflect the accuracy of the assignment of scans to experimental categories and as (2) confidence intervals which generalize standard hypothesis testing. When more than two categories are involved in the application of a pattern classifier, the use of confidence intervals for null hypothesis testing inflates Type I error. We address this problem with a Bonferonni-like correction. Our methodology is illustrated with the results of a pattern classifier described by O'Toole et al. (O'Toole, A., Jiang, F., Abdi, H., Haxby, J., 2005. Partially distributed representations of objects and faces in ventral temporal cortex. J. Cogn. Neurosci. 17, 580-590) who re-analyzed data originally collected by Haxby et al. (Haxby, J., Gobbini, M., Furey, M., Ishai, A., Schouten, J., Pietrini, P., 2001. Distributed and overlapping representation of faces and objects in ventral temporal cortex. Science 293, 2425-2430). {\textcopyright} 2008 Elsevier Inc. All rights reserved.},
author = {Abdi, Herv{\'{e}} and Dunlop, Joseph P. and Williams, Lynne J.},
doi = {10.1016/j.neuroimage.2008.11.008},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/A71-How to compute reliability estimates and display confidence and tolerance intervals for pattern classifiers using the Bootstrap and 3-way multidimensional scaling(DISTATIS).pdf:pdf},
issn = {10538119},
journal = {NeuroImage},
keywords = {Bonferonni correction,Bootstrap resampling,Confidence interval,DISTATIS,Group analysis,Multidimensional scaling (MDS),Pattern classifiers,RV coefficient,Reliability estimation,STATIS,Tolerance interval,{\v{S}}id{\`{a}}k correction},
number = {1},
pages = {89--95},
publisher = {Elsevier Inc.},
title = {{How to compute reliability estimates and display confidence and tolerance intervals for pattern classifiers using the Bootstrap and 3-way multidimensional scaling (DISTATIS)}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2008.11.008},
volume = {45},
year = {2009}
}
@misc{Abdi2010e,
address = {Thousand Oaks},
author = {Abdi, Herv{\'{e}} and Williams, Lynne J.},
booktitle = {Encyclopedia of Research Design},
chapter = {Barycentri},
editor = {Salkind, Neil},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/C74-BADIA.pdf:pdf},
publisher = {Sage},
title = {{Barycentric Discriminant Analysis (BADIA)}},
year = {2010}
}
@article{Bishop2019,
abstract = {Dorothy Bishop describes how threats to reproducibility, recognized but unaddressed for decades, might finally be brought under control. Dorothy Bishop describes how threats to reproducibility, recognized but unaddressed for decades, might finally be brought under control.},
author = {Bishop, Dorothy},
doi = {10.1038/d41586-019-01307-2},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/Bishop2019{\_}NatureIrreproducibility.pdf:pdf},
issn = {14764687},
journal = {Nature},
keywords = {Publishing,Research management},
number = {7753},
pages = {435},
title = {{Rein in the four horsemen of irreproducibility}},
volume = {568},
year = {2019}
}
@incollection{Abdi2007a,
abstract = {SVD and one kind of GSVD},
address = {Thousand Oaks},
author = {Abdi, Herv{\'{e}}},
booktitle = {Encyclopedia of Measurement and Statistics},
doi = {10.4135/9781412952644.n413},
editor = {Salkind, Neil},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/C34-SVD and GSVD.pdf:pdf},
pages = {1--14},
publisher = {Sage},
title = {{Singular and Generalized Singular Value Decomposition}},
year = {2007}
}
@misc{Abdi2010c,
address = {Thousand Oaks},
author = {Abdi, Herv{\'{e}} and Williams, Lynne J.},
booktitle = {Encyclopedia of Research Design},
chapter = {Jackknife},
editor = {Salkind, Neil},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/C61-Jackknife.pdf:pdf},
publisher = {Sage},
title = {{Jackknife}},
year = {2010}
}
@article{Abdi2010f,
author = {Abdi, Herv{\'{e}} and Williams, Lynne J.},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/A77-PCA.pdf:pdf},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
number = {August},
pages = {1--16},
title = {{Principal component analysis Tutorial Review}},
volume = {2},
year = {2010}
}
@article{Abdi2013,
abstract = {Multiple factor analysis (MFA, also called multiple factorial analysis) is an extension of principal component analysis (PCA) tailored to handle multiple data tables that measure sets of variables collected on the same observations, or, alternatively, (in dual-MFA) multiple data tables where the same variables are measured on different sets of observations. MFA proceeds in two steps: First it computes a PCA of each data table and 'normalizes' each data table by dividing all its elements by the first singular value obtained from its PCA. Second, all the normalized data tables are aggregated into a grand data table that is analyzed via a (non-normalized) PCA that gives a set of factor scores for the observations and loadings for the variables. In addition, MFA provides for each data table a set of partial factor scores for the observations that reflects the specific 'view-point' of this data table. Interestingly, the common factor scores could be obtained by replacing the original normalized data tables by the normalized factor scores obtained from the PCA of each of these tables. In this article, we present MFA, review recent extensions, and illustrate it with a detailed example. {\textcopyright} 2013 Wiley Periodicals, Inc.},
author = {Abdi, Herv{\'{e}} and Williams, Lynne J. and Valentin, Domininique},
doi = {10.1002/wics.1246},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/A101-Multiple factor analysis- principal component analysis for multitable and multiblock data sets.pdf:pdf},
issn = {19395108},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
keywords = {Barycentric discriminant analysis (BADA),Consensus PCA,Generalized Procrustes analysis (GPA),Generalized singular value decomposition,INDSCAL,Multiblock PCA,Multiblock barycentric discriminant analysis (MUDI,Multiblock correspondence analysis,Multiple factor analysis (MFA),Multiple factor analysis barycentric discriminant ,Multiple factorial analysis,Multitable PCA,Principal component analysis,STATIS},
number = {2},
pages = {149--179},
title = {{Multiple factor analysis: Principal component analysis for multitable and multiblock data sets}},
volume = {5},
year = {2013}
}
@article{Nguyen2019,
abstract = {Dimensionality reduction (DR) is frequently applied during the analysis ofhigh-dimensional data. Both a means ofdenoising and simplification, it can be beneficial for the majority of modern biological datasets, in which it's not uncommon to have hundreds or even millions of simultaneous measurements collected for a single sample. Because of“the curse ofdimension- ality,” many statistical methods lack power when applied to high-dimensional data. Even if the number ofcollected data points is large, they remain sparsely submerged in a voluminous high-dimensional space that is practically impossible to explore exhaustively (see chapter 12 [1]). By reducing the dimensionality ofthe data, you can often alleviate this challenging and troublesome phenomenon. Low-dimensional data representations that remove noise but retain the signal ofinterest can be instrumental in understanding hidden structures and pat- terns. Original high-dimensional data often contain measurements on uninformative or redundant variables. DR can be viewed as a method for latent feature extraction. It is also fre- quently used for data compression, exploration, and visualization. Although many DR tech- niques have been developed and implemented in standard data analytic pipelines, they are easy to misuse, and their results are often misinterpreted in practice. This article presents a set ofuseful guidelines for practitioners specifying how to correctly perform DR, interpret its out- put, and communicate results. Note that this is not a review article, and we recommend some important reviews in the references.},
author = {Nguyen, Lan Huong and Holmes, Susan},
doi = {10.1371/journal.pcbi.1006907},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/Supp1-[Nguyen 2019] Ten quick tips for effective dimensionality reduction.pdf:pdf},
isbn = {1111111111},
issn = {15537358},
journal = {PLoS computational biology},
number = {6},
pages = {e1006907},
title = {{Ten quick tips for effective dimensionality reduction}},
volume = {15},
year = {2019}
}
@article{Hesterberg2011,
abstract = {This article provides an introduction to the bootstrap. The bootstrap provides statistical inferences-standard error and bias estimates, confidence intervals, and hypothesis tests-without assumptions such as Normal distributions or equal variances. As such, bootstrap methods can be remarkably more accurate than classical inferences based on Normal or t distributions. The bootstrap uses the same basic procedure regardless of the statistic being calculated, without requiring the use of application-specific formulae. This article may provide two big surprises for many readers. The first is that the bootstrap shows that common t confidence intervals are woefully inaccurate when populations are skewed, with one-sided coverage levels off by factors of two or more, even for very large samples. The second is that the number of bootstrap samples required is much larger than generally realized. WIREs Comp Stat 2011 3 497-526 DOI: 10.1002/wics.182 For further resources related to this article, please visit the WIREs website Copyright {\textcopyright} 2011 John Wiley {\&} Sons, Inc.},
author = {Hesterberg, Tim},
doi = {10.1002/wics.182},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/Bootstrap{\_}Hesterberg.pdf:pdf},
isbn = {9781118596333},
issn = {19395108},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
keywords = {Bias,Inference,Permutation tests,Resampling,Standard error},
number = {6},
pages = {497--526},
title = {{Bootstrap}},
volume = {3},
year = {2011}
}
@misc{Abdi2010d,
address = {Thousand Oaks},
author = {Abdi, Herv{\'{e}} and Williams, Lynne J.},
booktitle = {Encyclopedia of Research Design},
chapter = {Correspond},
editor = {Salkind, Neil},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/C69-Correspondence analysis.pdf:pdf},
publisher = {Sage},
title = {{Correspondence Analysis}},
year = {2010}
}
@incollection{Abdi2003,
address = {Thousand Oaks},
author = {Abdi, Herv{\'{e}}},
booktitle = {Encyclopedia for research methods for the social sciences},
chapter = {Multivaria},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/C27-Multivariate analysis.pdf:pdf},
publisher = {Sage},
title = {{Multivariate Analysis}},
url = {https://personal.utdallas.edu/{~}herve/Abdi-MultivariateAnalysis-pretty.pdf},
year = {2003}
}
@incollection{Abdi2010b,
address = {Thousand Oaks},
author = {Abdi, Herv{\'{e}} and Williams, Lynne J.},
booktitle = {Encyclopedia of Research Design},
chapter = {Matrix Alg},
editor = {Salkind, Neil},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/C59-Matrix algebra.pdf:pdf},
publisher = {Sage},
title = {{Matrix Algebra}},
year = {2010}
}
@incollection{Abdi2009a,
address = {New York},
author = {Abdi, Herv{\'{e}} and Edelman, Betty and Valentin, Domininique and Dowling, W. Jay},
booktitle = {Experimental Design and Analysis for Psychology},
chapter = {23, 24},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/B7-Experimental design and analysis for psychology-Matrix algebra and the general linear model.pdf:pdf},
publisher = {Oxford University Press},
title = {{Experimental Design and Analysis for Psychology}},
url = {http://www.utdallas.edu/{~}herve/R{\_}comp4Abdi{\_}ExperimentalDesign.pdf{\%}5Cnhttp://www.utdallas.edu/{~}herve/SAS{\_}comp4Abdi{\_}ExperimentalDesign.pdf},
year = {2009}
}
@article{Williams2010,
author = {Williams, Lynne J and Abdi, Herv{\'{e}}e and French, Rebecca and Orange, Joseph B},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/A78-Tutorial on multiblock discriminant correspondence analysis(MUDICA)(1).pdf:pdf},
journal = {Journal of Speech, Language, and Hearing Research},
keywords = {discourse,discriminant analysis,inferential tool,multiblock discriminant correspondence analysis,qualitative data analysis},
number = {October},
pages = {1372--1393},
title = {{Correspondence Analysis ( MUDICA ): Data From Clinical Populations}},
volume = {53},
year = {2010}
}
@incollection{Abdi2007c,
address = {Thousand Oaks},
author = {Abdi, Herv{\'{e}}},
booktitle = {Encyclopedia of Measurement and Statistics},
chapter = {Discrimina},
doi = {10.4135/9781412952644.n140},
editor = {Salkind, Neil},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/C50-DiCA.pdf:pdf},
publisher = {Sage},
title = {{Discriminant Correspondence Analysis}},
year = {2007}
}
@misc{Abdi2010,
address = {Thousand Oaks},
author = {Abdi, Herve},
booktitle = {Encyclopedia of Research Design},
editor = {Salkind, Neil},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/C71-Congruence coefficient, Rv-coefficient, and mantel coefficient.pdf:pdf},
pages = {1--15},
publisher = {Sage},
title = {{Conguence: Congruence coefficient, Rv-coefficient, and Mantel coefficient}},
year = {2010}
}
@article{Leek2017,
abstract = {As debate rumbles on about how and how much poor statistics is to blame for poor reproducibility, Nature asked influential statisticians to recommend one change to improve science. The common theme? The problem is not our maths, but ourselves.},
author = {Leek, Jeff and McShane, Blakeley B. and Gelman, Andrew and Colquhoun, David and Nuijten, Mich{\`{e}}le B. and Goodman, Steven N.},
doi = {10.1038/d41586-017-07522-z},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/pvalue-NatureComment2017.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7682},
pages = {557--559},
title = {{Five ways to fix statistics}},
volume = {551},
year = {2017}
}
@incollection{Abdi2007,
address = {Thousand Oaks},
author = {Abdi, Herv{\'{e}}},
booktitle = {Encyclopedia of Measurement and Statistics},
chapter = {Metric Mul},
doi = {10.1.1.220.2654},
editor = {Salkind, Neil},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/C43-Matric multidimensional scaling.pdf:pdf},
isbn = {9781412916110},
pages = {598--605},
publisher = {Sage},
title = {{Metric Multidimensional Scaling (MDS): Analyzing Distance Matrices Multidimensional Scaling : Eigen-analysis of a distance matrix}},
year = {2007}
}
@article{Meng2016,
abstract = {State-of-the-art next-generation sequencing, transcriptomics, proteomics and other high-throughput 'omics' technologies enable the efficient generation of large experimental data sets. These data may yield unprecedented knowledge about molecular pathways in cells and their role in disease. Dimension reduction approaches have been widely used in exploratory analysis of single omics data sets. This review will focus on dimension reduction approaches for simultaneous exploratory analyses of multiple data sets. These methods extract the linear relationships that best explain the correlated structure across data sets, the variability both within and between variables (or observations) and may highlight data issues such as batch effects or outliers. We explore dimension reduction techniques as one of the emerging approaches for data integration, and how these can be applied to increase our understanding of biological systems in normal physiological function and disease.},
author = {Meng, Chen and Zeleznik, Oana A. and Thallinger, Gerhard G. and Kuster, Bernhard and Gholami, Amin M. and Culhane, Aed{\'{i}}n C.},
doi = {10.1093/bib/bbv108},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/Supp2-[Meng 2016] Dimension reduction techniques for the integrative analysis of multi-omics data.pdf:pdf},
issn = {14774054},
journal = {Briefings in Bioinformatics},
keywords = {Dimension reduction,Exploratory data analysis,Integrative genomics,Multi-assay,Multi-omics data integration,Multivariate analysis},
number = {4},
pages = {628--641},
title = {{Dimension reduction techniques for the integrative analysis of multi-omics data}},
volume = {17},
year = {2016}
}
@article{Abdi2006,
abstract = {In this paper we present a generalization of classical multidimensional scaling called DISTATIS which is a new method that can be used to compare algorithms when their outputs consist of distance matrices computed on the same set of objects. The method first evaluates the similarity between algorithms using a coefficient called the RV coefficient. From this analysis, a compromise matrix is computed which represents the best aggregate of the original matrices. In order to evaluate the differences between algorithms, the original distance matrices are then projected onto the compromise. We illustrate this method with a "toy example" in which four different "algorithms" (two computer programs and two sets of human observers) evaluate the similarity among faces.},
author = {Abdi, H. and O'Toole, A.J. and Valentin, D. and Edelman, B.},
doi = {10.1109/cvpr.2005.445},
file = {:C$\backslash$:/Users/Brendon/Documents/PhD Work/Classes/RM3/Reading/abdi-distatis2005.pdf:pdf},
pages = {42--42},
title = {{DISTATIS: The Analysis of Multiple Distance Matrices}},
year = {2006}
}